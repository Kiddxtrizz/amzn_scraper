import requests
from urllib.parse import urljoin
import time
from bs4 import BeautifulSoup


prdt_lnk = []

# List of User Agents
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux i686; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0'
]

# Index to track the current User Agent
user_agent_index = 0

# Make a request with a rotated User Agent
def make_request(url):
    global user_agent_index
    headers = {'User-Agent': user_agents[user_agent_index],
                'Accept-Language': 'da, en-gb, en',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Referer': 'https://www.google.com/'
                }
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Raise an exception if the response was not successful
    except requests.RequestException as e:
        print(f"Failed to make a request to {url}: {e}")
        return None
    
    user_agent_index = (user_agent_index + 1) % len(user_agents)
    return response.text

# Scrape product title from Amazon product page
url = 'https://www.amazon.com/Prescription-Medications-Generic/s?rh=n%3A19763358011%2Cp_n_feature_fourteen_browse-bin%3A19763604011'

while url:
    # Make a request to the page
    html_content = make_request(url)
    if html_content is None:
        break

    # Parse the page content
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find all product links on the page
    product_links = [a['href'] for a in soup.find_all('a', {'class': 'a-link-normal a-text-normal'})]

    # Add the product links to the list
    prdt_lnk += product_links

    # Find the link to the next page
    next_page_el = soup.select_one('a.s-pagination-next')
    if next_page_el:
        url = next_page_el.attrs.get('href')
        url = urljoin("https://amazon.com", url)
        print(f'Scraping next page: {url}', flush=True)
    else:
        url = None
    
    time.sleep(3) # Add a delay of 2 seconds between requests
    

from concurrent.futures import ThreadPoolExecutor, as_completed
from requests_html import HTMLSession
from bs4 import BeautifulSoup
import json
import re
import pandas as pd
from fp.fp import FreeProxy

# Initialize a proxy using the FreeProxy class
proxy = FreeProxy(country_id=['US'], https=True).get()
proxies = {
    'http': proxy,
    'https': proxy,
}

# Initialize session globally to be used across threads
session = HTMLSession()

# Function to scrape data from a single page with proxy support
def scrape_page(url, headers, proxies):
    try:
        # Get the page content using the proxy
        response = session.get(url, headers=headers, proxies=proxies)

        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.html.html, 'html.parser')

        # Extract JSON-like data using regex
        pattern = re.compile(r'window\.detailPageViewModelString\s*=\s*(\{.*?\});', re.DOTALL)
        script_tags = soup.find_all('script', {'type': 'text/javascript'})

        for script in script_tags:
            match = pattern.search(script.string or "")
            if match:
                json_data_str = match.group(1)
                json_data = json.loads(json_data_str)
                return extract_data_from_page(json_data)

    except Exception as e:
        print(f"Error scraping {url}: {e}")

    return None

# Function to clean and convert price strings to float values
def clean_price(price_str):
    # Remove non-numeric characters except for the period
    cleaned_str = re.sub(r'[^\d.]', '', price_str)
    # Ensure only one period remains by finding all occurrences and keeping the first
    if cleaned_str.count('.') > 1:
        parts = cleaned_str.split('.')
        cleaned_str = parts[0] + '.' + ''.join(parts[1:])

    return float(cleaned_str) if cleaned_str else None
# Function to extract data from a JSON object
def extract_data_from_page(json_data):
    # Extract drug name and separate strength
    full_drug_name = json_data['productMainDisplayViewModel']['productHeading']['displayString']
    drug_name_parts = re.match(r'^(.*?),\s*(.*)$', full_drug_name)
    drug_name = drug_name_parts.group(1) if drug_name_parts else full_drug_name
    drug_strength = drug_name_parts.group(2) if drug_name_parts else ''

    # Extract drug summary
    drug_summary = json_data['productDetailsViewModel']['description']['content']['displayString']

    # Extract price details
    buy_box_view_model = json_data.get('buyBoxViewModel', {})

    # Insurance Estimate View Model
    try:
        insurance_estimate_view_model = buy_box_view_model.get('insurancePriceViewModel', {}).get('insuranceEstimateViewModel', {})
        median_price_str = insurance_estimate_view_model.get('medianPrice', {}).get('displayString', '')
        median_price = clean_price(median_price_str)
    except

    # Starting Cash Price
    starting_cash_price_str = buy_box_view_model.get('pharmacyPriceViewModel', {}).get('startingCashPriceAmount', {}).get('asTextComponentViewModel', {}).get('displayString', '')
    starting_cash_price = clean_price(starting_cash_price_str)

    # Price Amount
    price_amount_str = buy_box_view_model.get('pharmacyPriceViewModel', {}).get('priceAmount', {}).get('displayString', '')
    price_amount = clean_price(price_amount_str)

    # Retail Strike-through Price
    retail_strike_through_str = buy_box_view_model.get('pharmacyPriceViewModel', {}).get('retailStrikeThroughPrice', {}).get('displayString', '')
    retail_strike_through = clean_price(retail_strike_through_str)

    # Save With Prime Calculation
    save_with_prime_calculation_without_percentage_str = buy_box_view_model.get('pharmacyPriceViewModel', {}).get('saveWithPrimeCalculationWithoutPercentage', {}).get('displayString', '')
    save_with_prime_calculation_without_percentage = clean_price(save_with_prime_calculation_without_percentage_str)

    save_with_prime_calculation_percentage_str = buy_box_view_model.get('pharmacyPriceViewModel', {}).get('saveWithPrimeCalculationPercentage', {}).get('displayString', '')
    save_with_prime_calculation_percentage = clean_price(save_with_prime_calculation_percentage_str)

    # Create a dictionary representing one row
    data = {
        'DrugName': drug_name,
        'DrugStrength': drug_strength,
        'DrugSummary': drug_summary,
        'InsurancePrice': median_price,
        'PrimePrice': price_amount,
        'RetailPrice': retail_strike_through,
        'PrimePriceWithoutPercentage': save_with_prime_calculation_without_percentage,
        'PrimePercentageDiscount': save_with_prime_calculation_percentage,
    }
    return data

# Parallel scraping function with rate limiting and proxy
def parallel_scraping(urls, headers, proxies, batch_size=10, delay=2):
    all_data = []
    for i in range(0, len(urls), batch_size):
        batch = urls[i:i + batch_size]
        with ThreadPoolExecutor(max_workers=len(batch)) as executor:
            futures = {executor.submit(scrape_page, url, headers, proxies): url for url in batch}

            for future in as_completed(futures):
                data = future.result()
                if data:
                    all_data.append(data)

        # Introduce a delay after each batch to implement rate limiting
        time.sleep(delay)

    return all_data

# Sample usage
headers={'User-Agent': user_agents[user_agent_index],
                'Accept-Language': 'da, en-gb, en',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Referer': 'https://www.google.com/'
                }

# Scrape data in parallel
all_data = parallel_scraping(prdt_lnk, headers, proxies, batch_size=5, delay=5)

# Convert the collected data to a DataFrame
df = pd.DataFrame(all_data)

# Save to CSV or process further
df.to_csv('pharmacy_prices.csv', index=False)

df.head()
